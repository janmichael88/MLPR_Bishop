{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.1\n",
    "Let the derivative of the $error function E$ with respect to vector $w$ equal 0 (i.e $\\frac{\\partial E}{\\partial w} = 0$ ) and this will be the solutionof $w = w_{i}$ which minizizes $E$. To solve this problem we will caculate the derivative of E with respect fo every $w_{i}$ and let them equal to 0 instead.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w} = \\sum_{n=1}^{N}[y(x_{n},w) - t_{n}]x_{n}^i = 0 \\\\\n",
    "\\sum_{n=1}^{N} y(x_{n},w)x_{n}^i  = \\sum_{n=1}^{N} x_{n}^{i}t_{n}\\\\\n",
    "\\sum_{n=1}^{N}(\\sum_{j=0}^{M} w_{j}x_{n}^{j})x_{n}^{i} = \\sum_{n=1}^{N} x_{n}^{i}t_{n} \\\\\n",
    "\\sum_{n=1}^{N}\\sum_{j=0}^{M} w_{j}x_{n}^{j+1} = \\sum_{n=1}^{N} x_{n}^{i}t_{n} \\\\\n",
    "\\sum_{j=1}^{M}\\sum_{n=1}^{N} x_{n}^{j+1}w_{j} = \\sum_{n=1}^{N} x_{n}^{i}t_{n} \\\\ \n",
    "\\end{align}\n",
    "\n",
    "If we denote $A_{ij} = \\sum_{n=1}^{N} x_{n}^{i+j}$ and $T_{i} = \\sum_{n=1}^{i}t_{n}$ as $\\sum_{j=0}^{M}A_{ij}w_{j}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.2\n",
    "Similar to 1.1, the only difference is the penalty term:\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w_{i}} = \\sum_{n=1}^{N}[y(x_{n},w) - t_{n}]x_{n}^i + \\lambda w_{i} = 0 \\\\\n",
    "\\sum_{j=0}^{M} \\sum_{n=1}^{N} x_{n}^{j+1} w_{j} + \\lambda w_{i} = \\sum_{n=1}^{N} x_{n}^{i}t_{n} \\\\\n",
    "\\sum_{j=0}^{M} [\\sum_{n=1}^{N} x_{n}^{j+1} + \\delta_{ij}]w_{j} = \\sum_{n=1}^{N} x_{n}^{i}t_{n} \\\\\n",
    "where \\\\\n",
    "\\delta_{ij}=\\begin{cases}\n",
    "    0, & j \\ne i\\\\\n",
    "    1, & j = i\n",
    "  \\end{cases} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.3\n",
    "This can solved using Bayes's theorem: The probability of selecting an apple $P(a)$:\n",
    "\n",
    "\\begin{align}\n",
    "P(a) = P(a|r)P(r) + P(a|b)P(b) + P(a|g)P(g) = \\frac{3}{10} \\times 0.2 + \\frac{1}{1} \\times 0.2 + \\frac{3}{10}\\times 0.6 = 0.34\n",
    "\\end{align}\n",
    "\n",
    "Based on Bayes's the probabilit of selecting an oraing coming from the green box or $P(g|o)$ is:\n",
    "\n",
    "\\begin{align}\n",
    "P(o|g) = \\frac{P(g|o)P(g)}{P(o)}\n",
    "\\end{align}\n",
    "\n",
    "We calculate the proablity of selecting an orange then as:\n",
    "\n",
    "\\begin{align}\n",
    "P(o) = P(o|r)P(r) + P(o|b)P(b) + P(o|g)P(g) = \\frac{3}{10}\\times0.2 + \\frac{1}{2}\\times0.2 + \\frac{3}{10}\\times0.6 = 0.36\n",
    "\\end{align}\n",
    "\n",
    "Now we can calculate the posterior:\n",
    "\\begin{align}\n",
    "P(g|o) = \\frac{P(o|g)P(g)}{P(o)} = \\frac{\\frac{3}{10}\\times0.6}{0.36} = 0.5\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.4\n",
    "First let's calculate the derivative of $P_{y}(y)$ with respect to $y$, according to (1.27):\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dp_{y}(y)}{dy} = \\frac{d(p_{x}(g(y))|g(y)|)}{dy} = \\frac{dp{x}(g(y))}{dy}|g(y)| + p_{x}(g(y))\\frac{d|g(y)|}{dy} \\\\(1)\n",
    "\\end{align}\n",
    "\n",
    "The first term can be simplified even further:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dp_{x}(g(y))}{dy}|g(y)| = \\frac{dp_{x}(g(y))}{dg(y)} \\frac{dg(y)}{dy}|g(y)| \\\\ (2)\n",
    "\\end{align}\n",
    "\n",
    "If $\\hat{x}$ is the maximum density over x, we can obtain:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dp_{x}}{dx}|_{\\hat{x}} = 0\n",
    "\\end{align}\n",
    "\n",
    "Therefore, when $y = \\hat{y},s.t.\\hat{x} = g(\\hat{y})$, the first term on the right side of (2) will be 0, leading the first term in (1) equals to 0. However, since there is a second term in (1), the derivative may not equal to 0. But when a linear transformaiton is applied, the second term in (1) will vanish $(e.g. x = ay + b)$. A simple example can be shown by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_{x}(x) = 2x, && x \\in[0,1] => \\hat{x} = 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And given that\n",
    "\\begin{align}\n",
    "x=sin(y)\n",
    "\\end{align}\n",
    "\n",
    "It must be the case that $p_{y}(y) = sin(2y)|cos(y)|,y\\in[0,\\frac{\\pi}{2}]$, which can be simplified to:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_{y}(y) = sin(2y), && x \\in[0,\\frac{\\pi}{2}] => \\hat{x} = \\frac{\\pi}{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However, it is obvious that: $\\hat{x} \\neq sin(\\hat{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.5\n",
    "Take advantage of the property of expectation:\n",
    "\\begin{align}\n",
    "var[f] = E[(f(x)-E[f(X)])^2] \\\\\n",
    "= E[f(x)^2 -2f(x)E[f(x)] + E[f(x)]^2] \\\\\n",
    "= E[f(x)^2] - 2E[f(x)]^2 + E[f(x)]^2 \\\\\n",
    "= E[f(x)^2] - E[f(x)]^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.6\n",
    "Based on 1.41 we only need to prove when x and y is independent, $E_{x,y}[xy] = E[x]E[y]$. Since $x$ and $y$ are independent, we have:\n",
    "\n",
    "\\begin{align}\n",
    "p(x,y) = p_{x}(x)p_{y}(y)\n",
    "\\end{align}\n",
    "\n",
    "Therefore:\n",
    "\\begin{align}\n",
    "\\int \\int xyp(x,y)dxdy = \\int\\int xyp_{x}(x)p_{y}ydxdy \\\\\n",
    "= (\\int xp_{x}(x)dx)(\\int yp_{y}(y)dy)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.7\n",
    "This should be take advantage using u substitution:\n",
    "\n",
    "\\begin{align}\n",
    "I^2 = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} exp(-\\frac{1}{2\\sigma^2}x^2 - \\frac{1}{2\\sigma^2}y^2)dxdy \\\\\n",
    "= \\int_{0}^{2\\pi} \\int_{0}^{+\\infty} exp(-\\frac{1}{2\\sigma^2}r^2)rdrd\\theta \\\\\n",
    "\\end{align}\n",
    "\n",
    "let\n",
    "\\begin{align}\n",
    "x = rcos\\theta, y = rsin\\theta \n",
    "\\end{align}\n",
    "\n",
    "Based on the fact that:\n",
    "\\begin{align}\n",
    "\\int_{0}^{+\\infty}exp(-\\frac{1}{2\\sigma^2})rdr = -\\sigma^2exp(-\\frac{r^2}{2\\sigma^2})\\rvert_{0}^{+\\infty} = \\sigma^2\n",
    "\\end{align}\n",
    "\n",
    "Therfore, I can be solved:\n",
    "\\begin{align}\n",
    "I^2 = \\int_{0}^{2\\pi}\\sigma^2d\\theta = 2\\pi\\sigma^2 \\\\\n",
    "=> I = \\sqrt{2\\pi}\\sigma\n",
    "\\end{align}\n",
    "\n",
    "And next we will show that gaussian distribution, $\\mathcal{N}(x|\\mu,\\sigma^2)$ is normalized or equals 1 in the limits\n",
    "\n",
    "\\begin{align}\n",
    "\\int_{-\\infty}^{+\\infty}\\mathcal{N}(x|\\mu,\\sigma^2)dx = \\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2)dx \\\\\n",
    "= \\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}y^2)dy  &&(y=x-\\mu) \\\\\n",
    "= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int_{-\\infty}^{\\infty}exp(-\\frac{1}{2\\sigma^2}y^2)dy \\\\\n",
    "= 1\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.8\n",
    "The first question will need the result from 1.7\n",
    "\\begin{align}\n",
    "\\int_{-\\infty}^{+\\infty}\\mathcal{N}(x|\\mu,\\sigma^2)dx = \\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2)dx \\\\\n",
    "= \\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}y^2)dy  &&(y=x-\\mu) \\\\\n",
    "= \\mu \\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2} y^2dy + \\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}y^2)ydy \\\\\n",
    "= \\mu + 0 \\\\\n",
    "= \\mu\n",
    "\\end{align}\n",
    "\n",
    "The second problem has already been given in the hint. Recall that:\n",
    "\\begin{align}\n",
    "\\frac{d}{dx} (fg) = f\\frac{d}{dx}g + g\\frac{d}{dx}f\n",
    "\\end{align}\n",
    "\n",
    "We differentiate both sizes of 1.127 with respect to $\\sigma^2$:\n",
    "\\begin{align}\n",
    "\\int_{-\\infty}^{\\infty}(-\\frac{1}{2\\sigma^2} + \\frac{(x-\\mu)^2}{2\\sigma^4}\\mathcal{N}(x|\\mu,\\sigma^2)dx = 0\n",
    "\\end{align}\n",
    "\n",
    "Provided the fact that $\\sigma \\neq 0 $ we can get:\n",
    "\n",
    "\\begin{align}\n",
    "\\int_{-\\infty}^{\\infty}\\mathcal{N}(x|\\mu,\\sigma^2)dx = \\int_{-\\infty}^{\\infty}\\sigma^2\\mathcal{N}(x|\\mu,\\sigma^2)dx =\\sigma^2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "So the equation above has actually proven (1.51), according to the definition:\n",
    "\\begin{align}\n",
    "var[x] = \\int_{-\\infty}^{\\infty}(x-\\mathbb{E}[x])^2\\mathcal{N}(x|\\mu,\\sigma^2)dx\\\\\n",
    "\\end{align}\n",
    "\n",
    "Where $\\mathbb{E}[x] = \\mu$ has already been proved. Therfore:\n",
    "\\begin{align}\n",
    "var[x] = \\sigma^2\n",
    "\\end{align}\n",
    "\n",
    "Finally\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[x^2] = var[x] + \\mathbb{E}[x]^2 = \\sigma^2 + \\mu^2\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.9 Solution\n",
    "Here we only focus on 1.52, since 1.52 is the general form of 1.42. The maximum distribution isknowns as it smode and we can obtain:\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial{x}}\\mathcal{N}(x|\\mu,\\Sigma) = \\frac{1}{2}[\\Sigma^{-1} + (\\Sigma^{-1})^T](x-\\mu)\\mathcal{N}(x|\\mu,\\Sigma) \\\\\n",
    "= -\\Sigma^{-1}(x-\\mu)\\mathcal{N}(x|\\mu,\\Sigma)\n",
    "\\end{align}\n",
    "\n",
    "Where we take advantage of:\n",
    "\\begin{align}\n",
    "\\frac{\\partial{x^T}}{\\partial{x}}Ax = (A + A^T)x && and && (\\Sigma^{-1})^T = (\\Sigma^{-1})\n",
    "\\end{align}\n",
    "\n",
    "Therefore:\n",
    "\\begin{align}\n",
    "\\text{only when} x=\\mu, \\frac{\\partial{\\mathcal{N}}}{\\partial{x}}(x|\\mu,\\Sigma) = 0\\\\\n",
    "\\end{align}\n",
    "\n",
    "You could also calculate the Hessian Matrix to prove that it is a maximum. However, here we find that the first derivative only has one root. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.9\n",
    "We will solve this problem based on the defitino of expecation, variation, and independence!\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[x+z] = \\int\\int(x+z)p(x,z)dxdz \\\\\n",
    "= \\int\\int(x+z)p(x)p(z)dxdz \\\\\n",
    "= \\int\\int xp(x)p(z)dxdz + \\int\\int zp(x)p(z)dxdz \\\\\n",
    "= \\int(\\int p(z)dz)xp(x)dx + \\int(\\int p(x)dx)zp(z)dz \\\\\n",
    "= \\int xp(x)dx + \\int xp(z)dz \\\\\n",
    "= \\mathbb{E}[x] + \\mathbb{E}[z]\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "= var[x+z] = \\int\\int(x + z - \\mathbb{E}[x+z])^2 p(x,z)dxdz \\\\\n",
    "= \\int \\int ((x+z)^2 - 2(x+z)\\mathbb{E}[x+z] + \\mathbb{E}^2 [x+z])p(x,z)dxdz \\\\\n",
    "= \\int \\int (x+z)^2 p(x,z)dxdz - 2\\mathbb{E}[x+z]\\int \\int (x+z)p(x,z)dxdz + \\mathbb{E}^2[x+z] \\\\\n",
    "= \\int \\int (x+z)^2 p(x,z)dxdz - \\mathbb{E}^2[x+z] \\\\\n",
    "= \\int \\int (x^2 +2xz + z^2)p(x)p(z)dxdz - \\mathbb{E}^2[x+z] \\\\\n",
    "= \\int(\\int p(z)dz)x^2p(x)dx + \\int \\int 2xzp(x)p(z)dxdz + \\int (\\int p(x)dx)z^2 p(z)dz - \\mathbb{E}^2[x+z]\\\\\n",
    "= \\mathbb{E}[x^2] + \\mathbb{E}[x^2] - \\mathbb{E}^2[x+z] + \\int \\int 2xzp(x)p(z)dxdz \\\\\n",
    "= \\mathbb{E}[x^2] + \\mathbb{E}[z^2] - (\\mathbb{E}[x]-\\mathbb{E}[z])^2 + \\int\\int 2xzp(x)p(z)dxdz \\\\\n",
    "= \\mathbb{E}[x^2] + \\mathbb{E}^2[x] + \\mathbb{E}[z^2] - \\mathbb{E}^2[z] - 2\\mathbb{E}[x]\\mathbb{E}[z] + 2 \\int\\int xzp(x)p(z)dxdz \\\\\n",
    "= var[x] + var[z] - 2\\mathbb{E}[x]\\mathbb{E}[z] + 2(\\int xp(x)dx)(\\int z p(z)dz) \\\\\n",
    "= var[x] + var[z] \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.11\n",
    "Based on prior knowledge that $\\mu_{ML}$ and $\\sigma_{ML}^2$, we will first calculate $\\mu_{ML}$ by taking the derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d}{d\\mu} ln(p(x|\\mu,\\sigma^2) = \\frac{1}{\\sigma^2} \\sum_{n=1}^{N} (x_{n} - \\mu) \\\\\n",
    "let: \\\\\n",
    "\\frac{d}{d\\mu} ln(p(x|\\mu,\\sigma^2) = 0 \\\\\n",
    "therefore: \\\\\n",
    "\\mu_{ML} = \\frac{1}{N} \\sum_{n=1}^{N} x_{n} \\\\\n",
    "because: \\\\\n",
    "\\frac{d}{d\\sigma^2} ln(p(x|\\mu,\\sigma^2) = \\frac{1}{2\\sigma^4}(\\sum_{n=1}^{N} (x_{n} - \\mu)^2 - N\\sigma^2 \\\\\n",
    "also:\n",
    "\\frac{d}{d\\sigma^2} ln(p(x|\\mu,\\sigma^2) = 0 \\\\\n",
    "therefore: \\\\\n",
    "\\sigma_{ML}^2 = \\frac{1}{N} \\sum_{n=1}^{N} (x_{n} - \\mu_{ML})^2\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.12\n",
    "Show that $\\mathbb{E}[x_{n}x_{m}] = \\mu^2 + I_{nm}\\sigma^2$:\n",
    "\n",
    "It is fairly straightforward for $\\mathbb{E}[\\mu_{ML}]$, with the prior knowledge that $x_{n}$ is i.i.d and it always obeys a gaussian\n",
    "\n",
    "$\\mathbb{E}[\\mu_{ML}] = \\mathbb{E}[\\frac{1}{N}\\sum_{n=1}^{N} x_{n}] = \\frac{1}{N} \\mathbb{E}[\\sum_{n=1}^{N} x_{n}] = \\mathbb{E}[x_{n}] =\\mu$\n",
    "\n",
    "For $\\mathbb{E}[\\sigma_{ML}^2]$, we need to take advantage of 1.56:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\sigma_{ML}^2] = \\mathbb{E}[\\frac{1}{N}\\sum_{n=1}^{N}(x_{n} - \\mu_{ML})] \\\\\n",
    "= \\frac{1}{N}\\mathbb{E}[\\sum_{n=1}^{N}(x_{n} - \\mu_{ML})^2] \\\\\n",
    "= \\frac{1}{N} \\mathbb{E}[\\sum_{n=1}^{N} (x_{n}^2 - 2x_{n}\\mu_{ML} + \\mu_{ML}^2)] \\\\\n",
    "= \\frac{1}{N}\\mathbb{E}[\\sum_{n=1}^{N}x_{n}^2] - \\frac{1}{N}\\mathbb{E}[\\sum_{n=1}^{N} 2x_{n}\\mu_{ML}] + \\frac{1}{N}\\mathbb{E}[\\sum_{n=1}^{N}\\mu_{ML}^2] \\\\\n",
    "\\mu^2 + \\sigma^2 - \\frac{2}{N}\\mathbb{E}[\\sum_{n=1}^{N}x_{n}(\\frac{1}{N}\\sum_{n=1}^{N} x_{n})] + \\mathbb{E}[\\mu_{ML}^2] \\\\\n",
    "\\mu^2 + \\sigma^2 -\\frac{2}{N}\\mathbb{E}[\\sum_{n=1}^{N} x_{n}(\\sum_{n=1}^{N}x_{n})] + \\mathbb{E}[(\\frac{1}{N}\\sum_{n=1}^{N} x_{n})^2)] \\\\\n",
    "\\mu^2 + \\sigma^2 -\\frac{2}{N^2} \\mathbb{E}[(\\sum_{n=1}^{N} x_{n})^2] + \\frac{1}{N^2} \\mathbb{E}[(\\sum_{n=1}^{N} x_{n})^2] \\\\\n",
    "\\mu^2 + \\sigma^2 - \\frac{1}{N^2} \\mathbb{E}[(\\sum_{n=1}^{N} x_{n})^2] \\\\\n",
    "= \\mu^2 + \\sigma^2 - \\frac{1}{N^2}[N(N\\mu^2 + \\sigma^2)] \\\\\n",
    "=> \\mathbb{E}[\\sigma_{ML}^2] = (\\frac{N-1}{N})\\sigma^2 \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.13\n",
    "Using the results from problem 1.12\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\sigma_{ML}^2] = \\mathbb{E}[\\frac{1}{N} \\sum_{n=1}^{N} (x_{n} - \\mu)^2] \\\\\n",
    "= \\frac{1}{N} \\mathbb{E}[\\sum_{n=1}^{N}(x_{n} - \\mu)^2] \\\\\n",
    "= \\frac{1}{N} \\mathbb{E}[\\sum_{n=1}^{N}(x_{n}^2 + 2x_{n}\\mu + \\mu^2)] \\\\\n",
    " = \\frac{1}{N}\\mathbb{E}[\\sum_{n=1}^{N} x_{n}^2] - \\frac{1}{N}\\mathbb{E}[\\sum_{n=1}^{N} 2x_{n}\\mu] + \\frac{1}{N}\\mathbb{E}[\\sum_{n=1}^{N}\\mu^2] \\\\\n",
    " = \\mu^2 + \\sigma^2 -\\frac{2\\mu}{N}\\mathbb{E}[\\sum_{n=1}^{N}x_{N}] +  \\mu^2 \\\\\n",
    " = \\mu^2 + \\sigma^2 - 2\\mu^2 + \\mu^2 \\\\\n",
    " = \\sigma^2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "The biggest difference between prob 1.12 and 1.13 is that the mean of a gaussian distribuiont is known previously in 1.13 and not in prob 1.12. The difference can be shown by following the equations:\n",
    "\n",
    "$\n",
    "\\mathbb{E}[\\mu^2] = \\mu^2 \\\\\n",
    "\\text{$\\mu$ is determined, its expectation is itself} \\\\\n",
    "\\mathbb{E}[\\mu_{ML}^2] = \\mathbb{E}[(\\frac{1}{N}\\sum_{n=1}^{N}x_{n})^2 = \\frac{1}{N^2}\\mathbb{E}[(\\sum_{n=1}^{N} x_{n})^2] = \\frac{1}{N^2}N(N\\mu^2 + \\sigma^2) = \\mu^2 + \\frac{\\sigma^2}{N} \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.14 \n",
    "The problem is quite similar to the fact that any function $f(x)$ can be written into the sum of an odd function and an even function. If we let:\n",
    "\n",
    "\\begin{align}\n",
    "w_{ij}^S = \\frac{w_{ij} + w_{ji}}{2} && and && w_{ij}^A = \\frac{w_{ij} - w_{ji}}{2}\n",
    "\\end{align}\n",
    "\n",
    "It is obvious that they satisfy the constraints describe in the problem which are:\n",
    "\\begin{align}\n",
    "w_{ij} = w_{ij}^S + w_{ij}^A, && w_{ij}^S = w_{ji}^S, && w_{ij}^A = -w_{ji}^A\n",
    "\\end{align}\n",
    "\n",
    "To prove 1.132 we only need to simplify it:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{D} \\sum_{j=1}^{D}w_{ij}x_{i}x{j} = \\sum_{i=1}^{D} \\sum_{j=1}^{D} (w_{ij}^S + w_{ij}^A) x_{i}x_{j} \\\\\n",
    "= \\sum_{i=1}^{D} \\sum_{j=1}^{D} w_{ij}^S x_{i} x_{j} + \\sum_{i=1}^{D} \\sum_{j=1}^{D} w_{ij}^A x_{i} x_{j} \\\\\n",
    "\\end{align}\n",
    "\n",
    "So we only need to prove that the second term equals 0 and here us a simly trick...lets prove twive of the second term equals 0 instead:\n",
    "\n",
    "\\begin{align}\n",
    "2 \\sum_{i=1}^{D} \\sum_{j=1}^{D} w_{ij}^A x_{i} x_{j} = \\sum_{i=1}^{D} \\sum_{j=1}^{D}(w_{ij}^A + w_{ij}^A)x_{i}x_{j} \\\\ \n",
    "\\sum_{i=1}^{D} \\sum_{j=1}^{D}(w_{ij}^A - w_{ji}^A)x_{i}x_{j} \\\\\n",
    "\\sum_{i=1}^{D} \\sum_{j=1}^{D}w_{ij}^Ax_{i}x_{j} - \\sum_{i=1}^{D} \\sum_{j=1}^{D}w_{ji}^Ax_{i}x_{j} \\\\\n",
    "\\sum_{i=1}^{D} \\sum_{j=1}^{D}w_{ij}^Ax_{i}x_{j} - \\sum_{j=1}^{D} \\sum_{i=1}^{D}w_{ji}^Ax_{i}x_{j} \\\\\n",
    "= 0\n",
    "\\end{align} \n",
    "\n",
    "We chose the coefficient matrix to be symmetric as described in the problem. Considering about the symmetry we can see that if and only if for $i=1,2,...D$ and $i\\leq j,w_{ij}$ the whole matrix will be determined. Hence the number of indepdent parameters is:\n",
    "\n",
    "\\begin{align}\n",
    "D + D - 1 + ... + 1 = \\frac{D(D+1)}{2} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.15\n",
    "\n",
    "Lets begin by creating a mapping function:\n",
    "\n",
    "\\begin{align}\n",
    "F(x_{i1},x_{i2}...x_{iM}) = x_{j1}x_{j2}...x_{jM} \\\\\n",
    "s.t \\bigcup_{k=1}^{M} x_{ik} = \\bigcup_{k=1}^{M}x_{jk} && and && x_{j1} \\geq x_{j2} \\geq x_{j3}... \\geq x_{jM}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Here is an example of what the mappping function F does when D=5 and M=4:\n",
    "\\begin{align}\n",
    "F(x_{5} x_{2} x_{3} x_{2}) = x_{5} x_{3} x_{2} x_{2} \\\\\n",
    "F(x_{1} x_{3} x_{3} x_{2}) = x_{3} x_{3} x_{2} x_{1} \\\\\n",
    "\\end{align}\n",
    "\n",
    "After introducing F, the solution will be very simple, base on the fact that F will not change the value of the term but only rearrange it:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i_{1} = 1}^{D} \\sum_{i_{2}=1}^{D}...\\sum_{i_{M}=1}^{D} w_{i_{1}i_{2}...i_{M}} x_{i1}x_{i2}x_{iM} = \\sum_{j_{1} = 1}^{D} \\sum_{j_{2}=1}^{D}...\\sum_{j_{M}=1}^{D} w_{j_{1}i_{2}...j_{M}} x_{j1}x_{j2}x_{jM} \\\\\n",
    "where \\begin{cases}\n",
    "    \\tilde{w}_{{j1,j2...jM}} = \\sum_{w \\in \n",
    "    \\Omega} w\\\\\n",
    "    \\Omega = \\{w_{i1,i2...iM} | F(x_{i1}x_{i2}...x_{iM}) = x_{j1}x_{j2}...x_{jM}, \\forall x_{i1}x_{i2}...x_{iM} \\}\n",
    "  \\end{cases} \\\\\n",
    "\\end{align}\n",
    "\n",
    "By far, we have alreadr prveon 1.134. By inudtion we should be able to prove 1.135. We will being by proving D =1. When D=1,1.134 will degeneate into $\\tilde{w}x_{1}^M$. It only has one term whose coef is governed by $\\tilde{w}$ regardless the value of M.\n",
    "\n",
    "Lets begin based on 1.134:\n",
    "\\begin{align}\n",
    "\\sum_{i_{1} =1}^{D+1}\\sum_{i_{2} =1}^{i_{1}}...\\sum_{i_{M} =1}^{i_{M-1}} \\tilde{w}_{i_{1},i_{2}...i_{M}}x_{i1}x_{i2}...x_{iM}\n",
    "\\end{align}\n",
    "\n",
    "We divide the above expression into two parts based on the first summation, the first part is made up os $i_{i} =1,2...D$ base on the secnod part $i_{1} = D+ 1$. After division, the first part corresponds to $n(D,M)$ and the scond part forrespodns to $n(D+1,M-1)$:\n",
    "\n",
    "\\begin{align}\n",
    "n(D+1,M) = n(D,M) + n(D+1,M-1)\n",
    "\\end{align}\n",
    "\n",
    "And givne the fact that 1.135 holds for D:\n",
    "\n",
    "\\begin{align}\n",
    "n(D,M) = \\sum_{i=1}^{D} n(i,M+1)\n",
    "\\end{align}\n",
    "\n",
    "We can substitute:\n",
    "\n",
    "\\begin{align}\n",
    "n(D+1,M) = \\sum_{i=1}^{D} n(i, M-1) + n(D+1,M-1) = \\sum_{i=1}^{D+1} n(i,M-1) \\\\\n",
    "\\end{align}\n",
    "\n",
    "We will prove 1.136 in a differnt but simple way. Lets rewrtie 1.135 in permutation and combinattion form\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{D} = C_{i+M-2}^{M-1} = C_{D+M-1}^{M} \\\\\n",
    "\\text{expand the summation}\\\\\n",
    "C_{M-1}^{M-1} + C_{M}^{M-1} + ... C_{D+M-2}^{M-1} = C_{D+M-1}^{M}\n",
    "\\end{align}\n",
    "\n",
    "Rewrite the first term on the left to $C_{M}^{M}, becasue C_{M-1}^{M-1} = C_{M}^{M} = 1$. We need to prove that:\n",
    "\n",
    "\\begin{align}\n",
    "C_{M}^{M} + C_{M}^{M-1} + ... C_{D+M-2}^{M-1} == C_{D+M-1}^{M}\\\\\n",
    "\\end{align}\n",
    "\n",
    "We take advantage of the probperty $C_{N}^{r} = C_{N-1}^{r} + C_{N-1}^{r-1}$. We can recursively combine the frist term and the second term on the left side nad have it equal the right.\n",
    "\n",
    "Lets give som intuitve concepts by illustartiong when $M=0,1,2$. When M=0, 1.134 will consist of only a constant term, which means $n(D,0) = 1$. When $M=1$, it is obvious that $n(D,1) =D$ since we only have D terms if expanded. Wehn M = 2, it becomes $n(D,2) = \\frac{D(D+1)}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.16\n",
    "Same wasy as probem 1.15\n",
    "\n",
    "Here are the end results:\n",
    "\\begin{align}\n",
    "N(D,M+1) = N(D,M) + n(D,M+1)\n",
    "\\end{align}\n",
    "N is th totle nmber of idependent parameters given D and M and n is the number of independend parameters  in the term of order m.\n",
    "\n",
    "\\begin{align}\n",
    "N(D,M+1) = \\sum_{m=0}^{M+1} n(D,m)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "N(D,M) = \\frac{(D+M)!}{D!M!} \\approx \\frac{(D+M)^{D+M}}{D^{D}M^{M}} \\\\\n",
    "\\text{using stirling's approx $n! \\approx n^{n}e^{-n}$} \\\\\n",
    "\\frac{e^{M}}{M^{M}}D^{M}\n",
    "\\end{align}\n",
    "\n",
    "Here are the results for $N(10,3)$ and $N(100,3)$:\n",
    "\n",
    "\\begin{align}\n",
    "N(10,3) =  C_{13}^{3} = 286 \\\\\n",
    "N(100,3) = C_{103}^{3} = 176851 \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probelm 1.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\Gamma(x+1) = \\int_{0}^{-\\infty} u^{x} e^{-u} du\\\\\n",
    "= \\int_{0}^{-\\infty} -u^x de^{-u} \\\\\n",
    "= -u^{x}e^{-u} \\Big|_{0}^{+\\infty} - \\int_{0}^{+\\infty} e^{-u}d(-u^{x}) \\\\\n",
    "= -u^{x}e^{-u} \\Big|_{0}^{+\\infty} + x\\int_{0}^{+\\infty} e^{-u}d(-u^{x-1}) \\\\\n",
    "= -u^{x}e^{-u} \\Big|_{0}^{+\\infty} + x\\Gamma(x) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Using L'hospitals\"\n",
    "\\begin{align}\n",
    "\\lim_{u\\to+\\infty} -\\frac{u^{x}}{e^{u}} = \\lim_{u\\to+\\infty} - \\frac{x!}{e^{u}} = 0 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Wehn $u=0, -u^{x}e^{u} = 0$, so we have prove $\\Gamma(x+1) = x\\Gamma(x)$. Based on teh defition of $\\Gamma(x)$ we can write:\n",
    "\n",
    "\\begin{align}\n",
    "\\Gamma(1) = \\int_{0}^{+\\infty} e^{-u} du = -e^{-u} \\Big|_{0}^{+\\infty} = -(0-1) = 1\\\\\n",
    "\\end{align}\n",
    "\n",
    "There for when x is an interger:\n",
    "\n",
    "\\begin{align}\n",
    "\\Gamma(x) = (x-1)\\Gamma(x-1) = (x-1)(x-2)\\Gamma(x-2) = ... x!\\Gamma(1) = x!\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.18\n",
    "Based on 1.124 and 1.126 and by using $x$ to $\\sqrt(\\sigma)y$ one can obtain the exrpession:\n",
    "\n",
    "\\begin{align}\n",
    "\\int_{-\\infty}^{+\\infty} e^{x_{i}^2} dx_{i} = \\pi \\\\\n",
    "\\end{align}\n",
    "\n",
    "There for, the left side pf 1.42 will be $\\pi^{D/2}$. For the right side of 1.42:\n",
    "\n",
    "\\begin{align}\n",
    "S_{D} \\int_{0}^{+\\infty} e^{{-r}^2} r^{D-1} dr = S_{D} \\int_{0}^{+\\infty} e^{-u} u^{\\frac{D-1}{2}} d \\sqrt{u} && (u = r^2) \\\\\n",
    "= \\frac{S_D}{2} \\int_{0}^{+\\infty} e^{-u} D^{\\frac{D}{2} -1} du \\\\\n",
    "= \\frac{S_{D}}{2} \\Gamma(\\frac{D}{2})\\\\\n",
    "\\end{align}\n",
    "\n",
    "We obtain:\n",
    "\\begin{align}\n",
    "\\pi^{\\frac{D}{2}} = \\frac{S_{D}}{2} \\Gamma(\\frac{2}{2}) && => && S_{D} = \\frac{2\\pi^{\\frac{D}{2}}}{\\Gamma(\\frac{D}{2})} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Consider the relationshop between V and S of a spehre with ariratry radius in dimesnion D, we can obtain:\n",
    "\n",
    "\\begin{align}\n",
    "V = \\int S dr = \\int S_{D}r^{D-1} dr = \\frac{S_{D}}{D} r^{D}\n",
    "\\end{align}\n",
    "\n",
    "This give the expression of the volumne of a psher with radius r in dimension D, let r=1:\n",
    "\n",
    "\\begin{align}\n",
    "V_{D} =\\frac{S_{D}}{D}\\\\\n",
    "\\text{For $D=2$ and $D=3$} \\\\\n",
    "V_{2} = \\frac{S_{2}}{2} = \\frac{1}{2}\\frac{2\\pi}{\\Gamma(1)} = \\pi \\\\\n",
    "V_{3} = \\frac{S_{3}}{3} = \\frac{1}{3}\\frac{2\\pi^{1.5}}{\\Gamma(1.5)} \\\\\n",
    "=\\frac{4}{3} \\\\\n",
    "\\end{align}\n",
    "\n",
    "For two dimensions, its just $\\pi$ and for three its 4/3 of pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.19\n",
    "\n",
    "From the hint: the volumne of a sphere with radius r is $V_{D}\\cdot r^{D}$. Which is quite similar with the conclusion we obtained in Prob1.18, about the surface area except that it is proportional to the Dth power of its radius r^D\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{V_{sphere}}{V_{cube}} = \\frac{V_{D}a^{D}}{(2a)^{D}} = \\frac{S_{D}}{2^{D}D} =\n",
    "\\frac{\\pi^{D/2}}{2^{D-1}D\\Gamma(\\frac{D}{2})}\\\\\n",
    "\\end{align}\n",
    "\n",
    "WHere we have use the result of 1.143, and when $D\\to+\\infty$ we use a simple method show convergence to 0:\n",
    "\n",
    "\\begin{align}\n",
    "(1) = \\frac{2}{D}\\cdot(\\frac{\\pi}{4})^\\frac{D}{2}\\cdot\\frac{1}{\\Gamma(\\frac{D}{2})} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.20\n",
    "THe densit of the probailit in a thin shell with radius r and thickness $\\epsilon$ can be viewed as a constant. And considering that a sphere in dimension D with radius r has surface area $D_{D}r^{D-1}$, which has alreayd been proved in 1.19\n",
    "\n",
    "\\begin{align}\n",
    "\\int_{shell} p(x)dx = \\int_{shell} dx = \\frac{exp^{-\\frac{r^2}{2\\sigma^2}}}{(2\\pi \\sigma^2)^\\frac{D}{2}} S_{D}r^{D-1}\\epsilon \\\\\n",
    "\\end{align}\n",
    "\n",
    "We can denote: \n",
    "\n",
    "\\begin{align}\n",
    "p(r) = \\frac{S_{D}r^{D-1}}{(2\\pi\\sigma^2)^\\frac{D}{2}} exp(-\\frac{r^2}{2\\sigma^2}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "We set the derivattive equal to 0, and we will obtain its unique root(stationary point) $\\tilde{r} = \\sqrt{D-1}\\sigma$ because $r\\in[0,+\\infty]$. When $r\\lt \\tilde{r}$ the derivative is larger than 0, so p(r) will increase with r. The same can be said as for the opposte. THerefore r will be the only max point .\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{p(\\tilde{r} + \\epsilon)}{p(\\tilde{r})} =\n",
    "exp(-\\frac{2\\epsilon\\tilde{r} + \\epsilon^2}{2\\sigma^2} + (D-1)ln(1+\\frac{\\epsilon}{\\tilde{r}})\n",
    "\\end{align}\n",
    "\n",
    "We can approx the exp term using Taylor theorms\n",
    "\n",
    "\\begin{align}\n",
    " = - \\frac{\\epsilon}{\\sigma^2} \\\\\n",
    "\\end{align}\n",
    "\n",
    "So $p(\\tilde{r} + \\epsilon) = p(\\tilde{r}) exp(-\\frac{\\epsilon}{\\sigma^2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.21\n",
    "We let: \n",
    "\n",
    "\\begin{align}\n",
    "(ab)^\\frac{1}{2} - a = a^\\frac{1}{2}(b^\\frac{1}{2} - a^\\frac{1}{2}) \\geq 0 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Where we take advantage of $b\\geq b \\geq 0$:\n",
    "\\begin{align}\n",
    "p(mistake) = p(x \\in R_1,R_2) + P(x \\in R_2,R_2)  \\\\\n",
    "\\int_{R1} p(x,C_2) dx + \\int_{R2} p(x,C_1) dx \\\\\n",
    "\\end{align}\n",
    "\n",
    "Recall the deicision rule is that if $p(x,C_1) > p(x,C_2)$ for x, we assign it to $C_1$\n",
    "\n",
    "So: \n",
    "\n",
    "\\begin{align}\n",
    "\\int_{R1} p(x,C_2) dx \\leq \\int_{R1} (p(x,C_1)p(x,C_2))^\\frac{1}{2} dx \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.22\n",
    "\n",
    "We need to deeply understand 1.81, where $L_{kj} = 1 - L_{kj}$ is given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{k} L_{kj} p(C_{k} | x) = \\sum_{k} p(C_{k} | x) - p(C_{j} |x) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Given a specific x, the first term on the right side is a constant, which is 1 no matter what class $C_{j}$ we assign x to. THere if we want to minimuze the loss, we will maximize $p(C_{j} |x)$. So we assign x to class $C_{j}$ which can give the biggest posterior proabibility of $p(C_{j} | x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.23\n",
    "\\begin{align}\n",
    "\\mathbb{E} = \\sum_{k}\\sum_{j} \\int_{R_{j}} L_{kj} p(x,C_{k}) dx = \\sum_{k} \\sum_{j} \\int_{R_{j}} L_{kj}p(C_{k})p(x|C_{k}) dx \\\\\n",
    "\\end{align}\n",
    "\n",
    "If we denote a new loss matrix by: $L_{jk}^* = L_{jk}p(C_{k})$ we can obtain a new equation:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[L] = \\sum_{k}\\sum_{j} \\int_{R_j} L_{jk}^* p(C_{k}) dx \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.24\n",
    "The description of the problem is a little confusing, and what it really meands is that $\\lambda$ is the parameter governing the loss, just like theta governing the posterior probabiltiy $p(C_k | x)$ when we introduce the reject option. Therefore the reject option can be written in a new way when we view it from the view of lambda and the lass:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{choices} \\begin{cases}\n",
    "\\text{class} C_{j} && \\min_{l} \\sum_{k} L_{kl} p(C_{k} | x ) < \\lambda\\\\\n",
    "\\text{reject} && else\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Where $C_{j}$ is the clas that can obtain the minimum. If $L_{kj} = 1 - I_{jk}$ according to what we phave prove in 1.22, then it follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{k} p(C_{k} | x) = \\sum_{k} p(C_{k}|x) - p(C_{j} | x) = 1 - p(C_{j} |x)\n",
    "\\end{align}\n",
    "\n",
    "Therefore the reject criterion from the view of large lambda, above is actually equivalent to the largest posterior probabilit larger than $1-\\lambda$:\n",
    "\n",
    "\\begin{align}\n",
    "min_{l} \\sum_{k} L_{kl} P(C_{k} | x) && <=> && max_{l} p(C_{l} |x) > 1 - \\lambda \\\\\n",
    "\\end{align}\n",
    "\n",
    "And from the view of $\\theta$ and posterior probability, we label a class for x is given by the contraint:\n",
    "\n",
    "\\begin{align}\n",
    "max_{l} p(C_{l} | x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.25\n",
    "\n",
    "We can prove this informaly by dealing with one dimenion one at a time just as the same process in 1.87 and 1.89. Due to the face that the ttoal loss $E$ can be divided to the summaiton of loss on every dimension, and whats more they are independent. Here, we will use a more informal way to prove this. \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E} = \\int \\int [\\bf{y(x) -t)^{2}}p(\\bf{x,t})] dtdx\\\\\n",
    "\\frac{\\partial\\mathbb{E}[L]}{\\partial y(\\bf{x})} = 2 \\int [y(x) - t]p(x,t)dt = 0 \\\\\n",
    "=> y(x) = \\frac{\\int tp(x,t) dt}{p(x)} = \\mathbb{E}_{t}(t|x) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.26\n",
    "\n",
    "The process is identical as the deuction fpr 1.90. This is not repeated here, and we empahize that $\\mathbb{E}[t|x]$ is a funciton of x, and not t. Thus the integra will over t an x, and this can be simplified using integration by parts. We have:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[L] = \\int [y(x) - \\mathbb{E}[t|x]]^2p(x)d(x) + \\int \\mathbb{E}[t|x-t]^2 p(x,t)dxdt\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.27\n",
    "\n",
    "We deal using calculus of variations:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial\\mathbb{E}[L_{q}]}{\\partial y(x)} = q \\int [y(x-t)]^{q-1} sign(y(x)-t)p(x,t)dt = 0 \\\\\n",
    "\\int_{-\\infty}^{y(x)} [y(x) -t]^{q-1}p(x,t)dt = \\int_{y(x)}^{+\\infty} [y(x) - t]^{q-1} p(x,t)dt \\\\\n",
    "\\int_{-\\infty}^{y(x)} [y(x) -t]^{q-1}p(t|x)dt = \\int_{y(x)}^{+\\infty} [y(x) - t]^{q-1} p(t|x)dt \\\\\n",
    "\\end{align}\n",
    "\n",
    "Where we take advantage of $p(x,t) = p(t|x)p(x)$ and the property of sign function. So when q=1, it should be reduced to:\n",
    "\n",
    "\\begin{align}\n",
    "\\int_{-\\infty}^{+\\infty} p(t|x) dt = \\int_{y(x)}^{+\\infty} p(t|x)dt \\\\\n",
    "\\end{align}\n",
    "\n",
    "In other words, when q=1, the optimal y(x) will be ginve by the conditonal median. When q=0. it is non trivial. We need to rewrote, 1.91\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[L_{q}] = \\int(\\int |y(x) -t|^{q} p(t|x)p(x)dt)dx \\\\\n",
    "\\int (p(x) \\int |y(x) - t|^{q} p(t|x) dt)dx && (*)\\\\\n",
    "\\end{align}\n",
    "\n",
    "If we want to minimize $mathbb{E}[L_{q}]$ we only need to minimize the integrand of (*)\n",
    "\n",
    "When $q=0, |y(x) - t|^q$ is close to 1 everywhere except in the neighborhood around t = y(x):\n",
    "\n",
    "\\begin{align}\n",
    "(**) = \\int_{\\mathscr{U}} p(t|x)dt - \\int_{\\mathscr{E}} (1-|y(x) - t|^{q} p(t|x) dt \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the problems will serve as an introduction to infomraiton theory. We will rewrite the problem more precuslty. In info theory, $h(\\cdot)$ is called the information content and denoted as $I(\\cdot)$. Here we will still use $h(\\cdot)$ for consistency. The whole problem is about the property of $h(x)$. Based on our knowledge that $h(\\cot)$ is a monotonic function of the probability $p(x)$, we can obtain:\n",
    "\n",
    "\\begin{align}\n",
    "h(x) = f(p(x)) \\\\\n",
    "\\end{align}\n",
    "\n",
    "The above means that the information we obtain for a specific value of a random variable $x$ is corealted with it occuring probabilty $p(x)$ and its relationship is given by a mapping function $f(\\cdot)$. Suppose $C$ is the intersection of two independent events A and B. The infomatino of the ven C occuring is the compound message of both indepedent events A and B occuring:\n",
    "\n",
    "\\begin{align}\n",
    "h(C) = h(A\\cap B) = h(A) + h(B) \\\\\n",
    "\\\\\n",
    "\\text{A and B are independent} \\\\\n",
    "\\\\\n",
    "P(C) = P(A)\\cdot P(B) \\\\\n",
    "\\\\\n",
    "\\text{We apply function $f(\\cdot)$}: \\\\\n",
    "\\\\\n",
    "f(P(C)) = f(P(A) \\cdot P(B)) \\\\\n",
    "\\\\\n",
    "\\text{Moreover, we can get} \\\\\n",
    "\\\\\n",
    "h(A) + h(B) = f(P(A)\\cdot P(B)) = f(p(A)) + f(p(B)) = f(p(A)\\cdot p(B)) \\\\\n",
    "\\end{align}\n",
    "\n",
    "We obtain an important property of function $f(\\cdot): f(x\\cdot y) = f(x) + f(y)$\n",
    "\n",
    "First, we chose $x=y$ and then it is obvious $f(x^2) = 2(f(x))$. Secondly it is obvious $f(x^n) = nf(x), n \\in  \\mathbb{N}$. We will now prove fo $n+1$:\n",
    "\n",
    "\\begin{align}\n",
    "f(x^{n+1}) = f(x^{n}) + f(x) = nf(x) + f(x) = (n+1)f(x) \\\\\n",
    "\\end{align}\n",
    "\n",
    "So $f(x^{n}) = nf(x), n \\in \\mathbb{N}$. For an interger M, we can rewrtie $x^x$ as $(x_{m}^{n}))^{m}$ and take advantage of the aforementioned stopes. We can obtain:\n",
    "\n",
    "\\begin{align}\n",
    "f(x^{n}) = f((x^{\\frac{n}{m}})^{m}) = mf(x^{\\frac{n}{m}}) \\\\\n",
    "f(x^{\\frac{n}{m}}) = \\frac{n}{m}f(x) \\\\\n",
    "\\end{align}\n",
    "\n",
    "For an abritray postive $x,x \\in \\mathbb{R}^{+}$ we can find two positive rational array $\\{y_{n}\\}$ and $\\{z_{n}\\}$ which satisfy..\n",
    "\n",
    "\\begin{align}\n",
    "y_{1} < y_{2} ... < y_{N} < x && \\text{and} && \\lim_{N \\to +\\infty} y_{N} = x \\\\\n",
    "z_{1} > z_{2} ... > z_{N} > x && \\text{and} && \\lim_{N \\to +\\infty} z_{N} = x \\\\\n",
    "\\end{align}\n",
    "\n",
    "Function $f(\\cdot)$ is monotonic:\n",
    "\n",
    "\\begin{align}\n",
    "y_{N}f(p) = f(p^{yN}) \\leq f(p^x) \\leq f(p^{zN}) = z_{N}f(p) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Wen $N \\to +\\infty$ we obtain $f(p^{x}) = xf(p), x \\in \\mathbb{R}^+$, we let $p=e$ and $y=e^{x}$\n",
    "\n",
    "\\begin{align}\n",
    "f(y) = ln(y)f(e)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.29\n",
    "\n",
    "The entropy for an M state discrete random variabile x can be written as:\n",
    "\n",
    "\\begin{align}\n",
    "H[x] = -\\sum_{i}^{M} \\lambda_{i}ln(\\lambda_{i}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\lambda_{i}$ is the probability that x choose state $i$/ Here we choose a concave functoin $f(\\cdot) = ln(\\cdot)$ and rewrite using Jenesens\n",
    "\n",
    "\\begin{align}\n",
    "ln(\\sum_{i}^{M} \\lambda_{i}x_{i}) \\geq \\sum_{i=1}^{M} \\lambda_{i} ln(x_{i}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "We choose $x_{i} = \\frac{1}{\\lambda_{i}}$ and simplify:\n",
    "\n",
    "\\begin{align}\n",
    "ln(M) \\geq = -\\sum_{i=1}^{M} \\lambda_{i}ln(\\lambda_{i}) = H[x] \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.30\n",
    "\n",
    "Bases on the definition:\n",
    "\n",
    "\\begin{align}\n",
    "ln(\\frac{p(x)}{q(x)}) = ln(\\frac{s}{\\sigma}) - [\\frac{1}{2\\sigma^2}(x-\\mu)^2 - \\frac{1}{2s^2}(x-m)^2] \\\\\n",
    "= ln(\\frac{s}{\\sigma}) - [(\\frac{1}{2\\sigma^2} - \\frac{1}{2s^2})x^2 - (\\frac{\\mu}{\\sigma^2} - \\frac{m}{s^2})x + (\\frac{\\mu^2}{2\\sigma^2} - \\frac{m^2}{2s^2}) ] \\\\\n",
    "\\end{align}\n",
    "\n",
    "Use the following derivation below to complete ther rest of the problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[x^2] = \\int x^2 \\mathscr{N}(x|\\mu,\\sigma^2)dx = \\mu^2 + \\sigma^2 \\\\\n",
    "\\mathbb{E}[x] = \\int x \\mathscr{N}(x|\\mu,\\sigma^2) dx = \\mu \\\\\n",
    "\\int \\mathscr{N}(x|\\mu,\\sigma^2) dx = 1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Now we derive..\n",
    "\n",
    "\\begin{align}\n",
    "KL(p||q) = - \\int p(x) ln(\\frac{q(x)}{p(x)}) dx \\\\\n",
    "= \\int \\mathscr{N}(x|\\mu,\\sigma)ln(\\frac{q(x)}{p(x)}) dx \\\\\n",
    "= ln(\\frac{s}{\\sigma}) - (\\frac{1}{2\\sigma^2} - \\frac{1}{2s^2})(\\mu^2 + \\sigma^2) + (\\frac{\\mu}{\\sigma^2} -\\frac{m}{s^2})\\mu - (\\frac{\\mu^2}{2\\sigma^2} - \\frac{m^2}{2s^2}) \\\\\n",
    "=\\ln(\\frac{s}{\\sigma}) + \\frac{\\sigma^2 + (\\mu - m)^2}{2s^2} - \\frac{1}{2} \\\\\n",
    "\\end{align}\n",
    "\n",
    "We will discuss the result in more detail. Firstly, if KL is defined in information theory, the first term of the result will be $log_{2}(\\frac{s}{\\sigma})$. Secondl,y we denot $x = \\frac{s}{\\sigma}$ KL distance can be written as:\n",
    "\n",
    "\\begin{align}\n",
    "KL(p||q) = ln(x) + \\frac{1}{2x^2} - \\frac{1}{2} + a, && \\text{where a}\n",
    "&& = \\frac{(\\mu - m)^2}{2s^2} \\\\\n",
    "\\end{align}\n",
    "\n",
    "We calculate the derivative of KL w.r.t x and set to 0:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial(KL)}{\\partial x} = \\frac{1}{x} - x^{-3} = 0 \\\\\n",
    "x = 1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "When x < 1, the derivateive is less than 0, and when x > 1, it is greater than 0, which makes x = 1 the global minimum. When $x = 1, KL(p||q) = \\alpha$. When $\\mu = m,a$ will achieve its min at 0. In this was we have shown that the KL divergence between two gaussian distribtions is not less than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.31\n",
    "\n",
    "Lets first calculate $H[x,y]$:\n",
    "\n",
    "\\begin{align}\n",
    "H[x,y] = - \\int \\int p(x,y)ln(p(x,y)) dx dy \\\\\n",
    "= \\int \\int p(x,y)ln p(x)dx dy - \\int \\int p(x,y) ln p(y|x) dx dy \\\\\n",
    "= -\\int p(x) ln p(x) dx - \\int \\int p(x,y) ln p(y|x) dx dy \\\\\n",
    "= H[x] + H[y|x] \\\\ \n",
    "\\end{align}\n",
    "\n",
    "Where we take advantae of $p(x,y) = p(x)p(y|x)$, $p(x,y)dy = p(x)$. Continuing....\n",
    "\n",
    "\\begin{align}\n",
    "H[x] + H[y] - H[x,y] = H[y] - H[y|x] \\\\\n",
    "= - \\int p(y) ln p(y) dy + \\int \\int p(x,y) ln p(y|x) dx dy \\\\\n",
    "= - \\int\\int p(x,y) ln p(y)dxdy + \\int \\int p(x,y) ln p(y|x) dxdy \\\\\n",
    "= - \\int \\int p(x,y) ln(\\frac{p(x)p(y)}{p(x,y)}) dx dy \\\\\n",
    "= KL(p(x,y)|| p(x)p(y)) = I(x,y) \\geq 0 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Recall the following properties:\n",
    " \n",
    "\\begin{align}\n",
    "p(y) = \\int p(x,y) dx \\\\\n",
    "\\frac{p(y)}{p(y|x)} = \\frac{p(x) p(y)}{p(x,y)}\\\\\n",
    "\\end{align}\n",
    "\n",
    "It is straightforward that if and only if and y are statistically indepedent,t eh eqaulity holds true. So:\n",
    "\n",
    "\\begin{align}\n",
    "H[x,y] = - \\int \\int p(x,y) ln p(x,y) dx dy \\\\\n",
    "= - \\int\\int p(x,y) ln p(x)dxdy - \\int\\int p(x,y) ln p(y) dxdy \\\\\n",
    "= - \\int p(x) ln p(x) dx - \\int \\int p(y) ln p(y) dy \\\\\n",
    "= H[x] + H[y] \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.32\n",
    "\n",
    "We must introduce the $Jacobian Determinant$:\n",
    "\n",
    "\\begin{align}\n",
    "H[y] = -\\int p(y) ln p(y) dy \\\\\n",
    "= -\\int\\frac{p(x)}{|A|} ln\\frac{p(x)}{|A|} \\frac{\\partial y}{\\partial x}dx \\\\\n",
    "= - \\int p(x) ln \\frac{p(x)}{|A|} dx \\\\\n",
    "= - \\int p(x)ln p(x) dx - \\int p(x) ln |frac{1}{|A|} dx \\\\\n",
    "= H[x] + ln |A| \\\\\n",
    "\\end{align}\n",
    "\n",
    "Note, we have takend advantage of the following equations:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial y}{\\partial x} = A && and && p(x) = p(y)|\\frac{\\partial y}{\\partial x}| = p(x) |A| \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.33\n",
    "\n",
    "Based on the defintion fo $Entropy$ we write:\n",
    "\n",
    "\\begin{align}\n",
    "H[y|x] = -\\sum_{x_i} \\sum_{y_i} p(x_i y_j) ln p(y_j | x_i) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Considering the property of probability, we can obtain that $0 \\leq p(x_j | x_i) \\leq 1, 0 \\leq p(x_i ,y_j) \\leq 1$. There fore we can see that $-p(x_i , y_j) ln p(y_j | x_i) \\geq 0$, when bettween 0 and 1. When $p(y_j | x_i) = 0$ provided with the fact that $\\lim_{p \\to 0} plnp = 0$ , we can see that $-p(x_i , y_j) ln p(y_j | x_i) = -p(x_i) p(y_j | x_i) ln p(y_j | x_i) \\approx 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $p(x) = \\mathscr(N) (\\mu,\\sigma^2)$, we can rewrite its entropy as...\n",
    "\n",
    "\\begin{align}\n",
    "H[x] = - \\int p(x) ln p(x) dx \\\\\n",
    "= -\\int p(x) ln(\\frac{1}{2\\pi\\sigma^2})dx - \\int p(x)-\\frac{(x-\\mu)^2}{2\\sigma^2} dx \\\\\n",
    "= -ln(\\frac{1}{2\\pi\\sigma^2}) + \\frac{\\sigma^2}{2\\sigma^2} \\\\\n",
    "= \\frac{1}{2} (1 + ln (2\\pi\\sigma^2)) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.36\n",
    "\n",
    "Here we should make it lcear that if the second derivative is striclty psotive, the function must be convex. Hoever, the conversve may not be true. More precisely we will prove that a convex function is equivalent to its second derivative is non negative by first considering Taylor Theormes:\n",
    "\n",
    "\\begin{align}\n",
    "f(x+\\epsilon) = f(x) + \\frac{f'(x)}{1!}\\epsilon+ \\frac{f''(x)}{2!} \\epsilon^2 + \\frac{f'''(x)}{3!}\\epsilon^3 + ...\n",
    "\\end{align}\n",
    "\n",
    "We can also obtain the expression for f''(x) as:\n",
    "\n",
    "\\begin{align}\n",
    "f''(x) = \\lim_{\\epsilon \\to 0} = \\frac{f(x+\\epsilon) + f(x-\\epsilon) -2f(x)}{\\epsilon^2} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Where $O(\\epsilon^4)$ is neglected and $f(x)# is convex, we can obtain:\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = f(\\frac{1}{2}(x+\\epsilon)) + \\frac{1}{2}(x-\\epsilon) \\leq \\frac{1}{2}f(x+\\epsilon) + \\frac{1}{2}f(x - \\epsilon) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Hence f''(x) >=0. The converse situation is a little bit more complex. we will use the lagrandian to rewrtie the taylor expansion:\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = f(x_0) + f'(x_0)(x-x_0) + \\frac{f''(x^*)}{2} (x-x_0) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Where $x^*$ lies between x and $x_0$. The last term is non negative for all x. We let $x_0 = \\lambda x_1 + (1-\\lambda)x_2$ and $x = x_1$:\n",
    "\n",
    "\\begin{align}\n",
    "f(x_1) = f(x_0) + (1 - lambda)(x_2 - x_1)f'(x_0) \\\\\n",
    "f(x_2) \\geq f(x_0) + \\lambda(x_2 - x_1)f'(x_0) \\\\\n",
    "\\lambda(f(x_1)) + (1 - \\lambda)f(x_2) \\geq f(\\lambda x_1 + (1-\\lambda)x_2) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.38\n",
    "\n",
    "\\begin{align}\n",
    "f(\\sum_{m=1}^{M} \\lambda_{m} x_{m}) = f(\\lambda_{M+1} x_{M+1} + (1-\\lambda_{M+1}) \\sum_{m=1}^{M} \\frac{\\lambda_{m}}{1-\\lambda_{M+1}} x_{m}) \\\\\n",
    "\\leq \\lambda_{M+1} f(x_{M+1}) + (1 - \\lambda_{M+1}) f(\\sum_{m=1}^{M} \\frac{\\lambda_{m}}{1-\\lambda_{M+1}} x_{M}) \\\\\n",
    "\\leq \\lambda_{M+1} f(x_{M+1}) + (1-\\lambda_{M+1}) \\sum_{m=1}^{M} \\frac{\\lambda_{m}}{1-\\lambda_{M+1}} f(x_m) \\\\\n",
    "\\leq \\sum_{m=1}^{M+1} \\lambda_{m} f(x_m) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.39\n",
    "\n",
    "\\begin{align}\n",
    "H[x] = - \\sum_{i} p(x_i)ln p(x_i) = \\frac{2}{3}ln \\frac{2}{3} - \\frac{1}{3} ln \\frac{1}{3} = 0.6365 \\\\\n",
    "H[y] = - \\sum_{i} p(y_i)ln p(y_i) = \\frac{2}{3}ln \\frac{2}{3} - \\frac{1}{3} ln \\frac{1}{3} = 0.6365 \\\\\n",
    "H[x,y] = - \\sum_{i} p(x_i,y_j) ln p(x_i,y_j) = -3\\times\\frac{1}{3} ln \\frac{1}{3} - 0 = 1.0986 \\\\\n",
    "H[x|y] = - \\sum_{ij} p(x_i, y_j) ln p(x_i|y_j) = -\\frac{1}{3} ln 1 - \\frac{1}{3} ln \\frac{1}{2} -\\frac{1}{3} ln \\frac{1}{2} = 0.4621 \\\\\n",
    "H[y|x] = - \\sum_{ij} p(x_i, y_j) ln p(y_j|x_i) = -\\frac{1}{3} ln 1 - \\frac{1}{3} ln \\frac{1}{2} -\\frac{1}{3} ln \\frac{1}{2} = 0.4621 \\\\\n",
    "I[x,y] = - \\sum_{ij} p(x_i,y_j) ln \\frac{p(x_i)p(y_j)}{p(x_i,y_j)}\n",
    "\\end{align}\n",
    "\n",
    "So we have:\n",
    "\n",
    "\\begin{align}\n",
    "I[x,y] = H[x] - H[x|y] = H[y] - H[y|x]\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.40\n",
    "\n",
    "$f(x) = lnx$ is actually a strict concave function, there we take advantage of $Jensens In equality$:\n",
    "\n",
    "\\begin{align}\n",
    "f(\\sum_{i=1}^{M} \\lambda_m x_m) \\geq \\sum_{i=1}^{M} \\lambda_m f(x_m) \\\\\n",
    "\\end{align}\n",
    "\n",
    "We let $\\lambda_m = \\frac{1}{M}, m = 1,2...M$:\n",
    "\n",
    "\\begin{align}\n",
    "ln\\frac{x_1 + x_2 + ... +x_m}{M} \\geq \\frac{1}{M}[ln(x_1) + ln(x_2) + ... + ln(x_M)] = \\frac{1}{M}ln(x_1,x_2...x_M) \\\\\n",
    "\\frac{x_1 + x_2 + ... + x_m}{M} \\geq \\sqrt[\\leftroot{-2}\\uproot{2}M]{x_1,x_2,...x_M}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.41\n",
    "\n",
    "\\begin{align}\n",
    "I[x,y] = - \\int \\int p(x,y) ln \\frac{p(x) p(y)}{p(x,y)} dx dy \\\\\n",
    "= H[x] - H[x|y] \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
